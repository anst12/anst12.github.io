---
title: "트럼프 트위터 텍스트 전처리 및 기초통계량"
date : 2020-04-28
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE)
```

### 학습목표
- 문서-단어 행렬(DTM, Document-Text Matrix)을 만들고 이해한다.
- DTM을 통해 다음 기술통계분석을 실시한다.
  1) 말구름(word cloud)  
  2) 단어 간 상관관계 (word1이 발견되었을때 word2가 발견될 확률)  
  3) 문서 간 상관관계 (doc1과 doc2의 유사도)  
  4) 단어와 단어의 유사도 행렬을 바탕으로 군집분석(cluster analysis)  


```{r}
library(stringr)
library(tidyverse)
library(tm)
library(ggplot2)
```


### 예제 자료
다음 웹페이지에서는 트럼프 대통령의 전체 트윗을 저장하여 제공하고 있다(http://www.trumptwitterarchive.com).
이번 시간에는 트럼프 대통령의 트윗을 월별로 묶어 하나의 문서로 취급한 뒤,
트럼프 대통령의 트윗 단어 선택에는 어떤 특징이 나타나는지 살펴보고자 한다.


```{r}
trump_tweet <- read.csv("Trump_Tweet(0416).csv", header = TRUE, encoding = "UTF-8")
head(trump_tweet, 5)
```

```{r}
# 자료구조 확인
str(trump_tweet)
```

트럼프의 트윗을 담고 있는 `text`변수를 character로,  
트윗 생성시간을 의미하는 `created_at`변수를 date로,  
리트윗 여부를 의미하는 `is_retweet`변수를 logical로(TRUE = 리트윗, FALSE = 트럼프의 고유 트윗) 변환할 필요가 있다.

```{r}
# text 변수를 character 로 변환
trump_tweet$text <- as.character(trump_tweet$text)

# created_at 변수를 date 로 변환
trump_tweet$created_at <- as.Date(trump_tweet$created_at, format = "%m-%d-%Y")

# is_retweet 변수를 logical 로 변환
trump_tweet$is_retweet <- as.logical(trump_tweet$is_retweet)
```


### 1. 전처리

- is_retweet을 이용하여 RT는 제외하기
- created_at을 이용하여 트윗 text 을 월별로 모으기 (문서 = 월)
- 월별 text의 내용을 정제하기  


#### 1.1) 전처리: RT 제외하기
`tidyverse` 패키지들 가운데 데이터 전처리를 용이하게 해주는 `dplyr`를 활용한다.
`dplyr`패키지 가운데 `filter()` 함수를 이용하면 원하는 조건을 가진 관측값만 선택할 수 있다. is_retweet 변수 값이 FALSE인 경우만 선택하면 트럼프가 직접 올린 트윗만 선택할 수 있다.


```{r}
# is_retweet == FALSE 만 선택하여 trump_tweet1 오브젝트에 저장
trump_tweet1 <- trump_tweet %>% dplyr::filter(is_retweet == FALSE)
```


#### 1.2) 전처리: 트윗 text를 월별로 모으기
트윗 자료의 생성시간 변수(`created_at`)는 트윗이 포스팅된 시간을 포함한 일자 정보를 담고있다.
이번 시간에는 트럼프의 트윗을 월 단위로 묶어 하나의 문서로 취급하고자 한다.
사실 트윗을 월단위로 묶어 하나의 문서로 취급하는 것은 많은 정보를 누락시키거나, 분석에서의 왜곡을 초래할 수 있다.  
연구자에게 구체적인 시간정보가 필요할 수 있는 경우로는 다음 두 가지 상황을 들 수 있다.  

1) 분석 목적과 내용에 따라 시간(hour:min:sec) 정보가 중요할 수 있다. 어떤 연구자가 급변하는 정치적 상황(e.g. 쿠데타, 무력분쟁 위기 등)에 대한 분석을 시도한다면 시간 정보가 필요할 수도 있다.  

2) 인간이 인위적으로 나눈 연/월/일 이라는 시간 단위가 정치적 사안과 정치인의 행동에 미치는 영향을 고려해야 한다.
시간의 흐름을 나타내는 연/월/일 단위는 지구의 자전과 공전에 따른 시간 변화와 관계를 갖지만, 시대와 지역에 따라 다르게 정의되었다는 점에서 얼마간은 인위적이다.  

*e.g. 1) 예수의 탄생에 따른 서력기원 vs. 헤지라를 기준으로 한 이슬람력*  
*e.g. 2) 그레고리력, 율리우스력 등 서양의 역법 vs. 중국 왕조와 황제의 탄생을 기준으로 한 동아시아권의 역법*  

그럼에도 불구하고 연구 대상에 따라 특정한 연/월/일은 특별한 의미를 지닐 수 있다.
예를 들어 정치인들은 신년을 맞아 새로운 정치적 이니셔티브를 발표하고 강조할 수 있다(e.g. 북한 신년사 등). 5.18 광주 민주화운동, 6.25 전쟁 등 특정한 사건이 벌어진 일자에 해당 사건과 관련된 조사가 활기를 띄거나, 정책이 발표될 수도 있다. 즉 연구자가 관심을 가지는 주제의 성격에 따라 날짜가 의미를 가질 수 있다.

*cf. 미국 대통령들은 임기 초반 100일 간 공약 달성을 위해 가장 적극적으로 행동하기도 한다. 100일이라는 시간은 3달(약 90일)이라는 시간 단위와 일치하지도 않고, 어떤 정책이 실제로 수립되고 정착되기에 부족한 기간일 수 있지만, 100이라는 숫자가 동서고금을 막론하고 가지는 상징성으로 인해 의미를 가진다.*  

이처럼 연구의 목적에 따라 분석 단위를 명확히 설정해야 할 필요가 있지만, 이번 시간에는 트럼프 대통령이 트위터에서 사용하는 어휘의 특징을 개략적으로 살펴보기 위해 월(month)을 분석 단위로 삼는다. 트윗에서 나타나는 어휘 변화를 살펴보기에 1년이나 분기(3 month) 단위는 너무 길고, 1일이나 1주 단위는 너무 짧을 것 같다는 저자의 주관적인 판단이 들어가 있다. 또한 문서X단어 매트릭스를 통해 텍스트 자료의 기술통계분석을 보여주기에는 월별 분류가 적절할 것 같다고 생각하였다. 다른 시간 분류가 보다 적절하다고 여기는 분들도 계시겠지만, 이번 시간에는 저자의 분류에 따라주시기를 부탁드린다.  


먼저 Date 형태의 `created_at`변수를 문자형으로 변환한 뒤 `month`변수에 새롭게 저장한다. 그 다음 `str_extract()`함수를 이용해 '숫자 4개로 구성된 연도, 대시(-), 숫자 2개로 구성된 월' 문자열만 추출한다.  

```{r}
# str_extract 함수 사용을 위해 character로 형변환
trump_tweet1$month <- as.character(trump_tweet1$created_at) 
# YYYY-MM 형태로 월 표시
trump_tweet1$month <- str_extract(trump_tweet1$month, "[[:alnum:]]{4}-[[:alnum:]]{2}")
head(trump_tweet1$month, 10)
```


자료를 간결히 하기 위해 `select()`함수를 이용하여 `trump_tweet1`에서 `month`변수, `text`변수만 추출한다. 그 다음 `group_by()`함수를 이용하여 월별 자료를 그룹화 한다. 마지막으로 `summarise()`함수 및 `paste()`함수를 이용하여 `text`변수에 담긴 트럼프 월별 트윗을 하나로 붙인 뒤 `tweet`이라는 변수명으로 저장한다.

```{r}
# Trump 대통령 트윗 월별 자료 수집
trump_monthly <- trump_tweet1 %>%
  select(month, text) %>%
  group_by(month) %>%
  summarise(tweet = paste(text, collapse = " "))

# 데이터 처음 5행, 마지막 5행을 살펴보기
head(trump_monthly, 5); tail(trump_monthly, 5)
```



#### 1.3) 전처리: 텍스트 클렌징 
이제 트럼프 대통령의 트윗이 월별로 준비되었다. 텍스트 분석에 적합한 형태로 자료를 정제해야 한다.
트럼프 대통령의 2020년 4월 트윗 자료를 살펴보며 어떤 형태로 정제가 필요한지 구상해보자.

```{r}
# 2020년 4월 자료만 관찰하고 싶다면?  
length(trump_monthly$month)  # 자료는 총 132개의 월별 트윗을 가짐

# 1) 인덱싱
trump_monthly$tweet[132]

# 2) filter() 함수
trump_monthly %>% filter(month == "2020-04")
```

전처리를 위해 먼저 트위터의 특성을 고려해야 한다.  
트위터의 해시태그(#) 기능은 특정한 표현에 대한 관심을 표현하거나, 의미를 부여하는 수단으로 사용될 수 있다. 트위터 유저들은 해시태그를 통해 해당 표현이 사용된 다른 트윗을 검색할 수도 있고, 해당 표현이 의미하는 사회현상을 알릴 수도 있다. 예를 들어 #MeToo는 하비 와인스틴의 성추문을 폭로하는 과정에서 대중화 되어, 직장 내 성폭행 문제를 고발하고 근절하기 위한 미투 운동의 수단으로 사용되고 있다. 트럼프 대통령의 트윗에서는 그의 대표적인 슬로건인 'Make America Great Again'의 이니셜을 이용한 #MAGA 라는 표현을 자주 찾아볼 수 있다.
다음으로 골뱅이표(@, at sign)는 유저를 특정하는 기능을 한다(@사용자이름). 트위터 유저들은 @를 통해 특정 사용자의 트윗에 대해 답장하거나, 

다음 순서를 거쳐 트럼프 대통령 트윗에 대한 전처리를 한다.  
  1) 특수문자 처리  
    *예외처리: 해시태그(#)가 붙은 단어는 특별한 의미*  
    *예외처리: 골뱅이표(@)가 붙은 단어는 특정 인물 및 단체*  
  2) 대문자 -> 소문자 전환
    *예외처리: WHO(world health organization) = who?*
  3) 링크 제거 
  4) 숫자 제거  
  5) 특수문자 처리  
  6) 불용단어 처리  
  7) 어근동일화  

##### 1.3.1) 특수문자 처리
먼저 해시태그(#, hash tag)를 hstg로 변환한다.
```{r}
# 해시태그 변환
trump_monthly$tweet <- trump_monthly$tweet %>%
  str_replace_all("#", "hstg")
```

다음으로 골뱅이표(@, at sign)을 atsgn으로 변환한다.
```{r}
# 골뱅이표 변환
trump_monthly$tweet <- trump_monthly$tweet %>%
  str_replace_all("@", "atsgn")
```

##### 1.3.2) 대문자-소문자 전환
텍스트 분석을 위해 대문자르 소문자로 전환할 필요가 있다. 그런데 인명, 약어 등에 대한 예외처리 없이 일괄적으로 전환하게 되면 의미가 혼용되는 문제가 발생할 수 있다. 예를 들어 특별한 처리 없이 세계보건기구(WHO)를 소문자 변환하면 의문사(who)와 구분하지 못하는 문제가 생긴다 따라서 인명, 약어 등에 사용되는 대소문자와 일상적인 단어를 적절하게 처리하기 위해서는 연구자가 텍스트 전체를 읽어보거나, 텍스트 전체에 익숙한 상태여야 한다. 이번 시간에는 세계보건기구(WHO)와 의문사(who)를 구분하기 위해 세계보건기구 WHO를 worldhealthorg 로 전환한 뒤, 대문자를 소문자 변환하는 과정을 보인다.

```{r}
# 세계보건기구 WHO 를 worldhealthorg 로 변환하여 의문사 who와 구분하기
str_detect(trump_monthly$tweet, "WHO")  # WHO가 등장하는 월별 트윗

trump_monthly$tweet <- trump_monthly$tweet %>%
  str_replace_all("WHO", "worldhealthorg")  # WHO를 worldhealthorg로 변환

str_detect(trump_monthly$tweet, "worldhealthorg") 
```

```{r}
# 트럼프 트윗의 대문자를 소문자로 전환
trump_monthly$tweet <- tolower(trump_monthly$tweet)
```

##### 1.3.3) 링크 제거
트윗을 살펴보면 https://(또는 http://)로 시작하는 링크들을 발견할 수 있다. 트럼프가 인용하고자 하는 링크가 특별한 의미를 가질 수도 있지만, 이번 시간에는 트럼프가 직접 사용하는 어휘에 주목하므로 링크를 삭제한다.

```{r}
trump_monthly$tweet <- trump_monthly$tweet %>%
  str_remove_all("(https://)[[:graph:]]{1,}") %>%
  str_remove_all("(http://)[[:graph:]]{1,}")
```


##### 1.3.4) 숫자 제거
연구자의 목적에 따라 텍스트분석에서도 숫자가 의미를 가질 수 있다. 그러나 이번 시간에는 트럼프가 사용하는 단어와 표현에 주목하므로 모든 숫자를 삭제한다.

```{r}
trump_monthly$tweet <- trump_monthly$tweet %>%
  str_remove_all("[[:digit:]]")
```


##### 1.3.5) 특수문자 처리
트럼프 대통령은 트위터에서 따옴표("")와 느낌표(!)를 사용하여 특정 표현을 강조하는 경향이 있다. 트럼프 자신이 특정한 표현을 특별히 지정했다는 점에서 따옴표와 느낌표가 가지는 의미가 있지만, 이번 시간에는 모든 특수문자를 삭제하고 기본적인 분석을 시도하고자 한다. 

```{r}
trump_monthly$tweet <- trump_monthly$tweet %>%
  str_replace_all("[[:punct:]]", "")

trump_monthly$tweet <- trump_monthly$tweet %>%
  str_remove_all("amp")
```

##### 1.3.6) ngram
한 단어가 아니지만, 연이어 나오는 단어들이 의미를 가질 수 있다. White House를 구성하는 white와 house는 각각 흰색과 집이라는 의미를 가지고 있지만, 두 단어가 연달아 등장하면 미국 대통령 집무실이 있는 백악관을 의미할 수 있다(말 그대로 흰집을 의미할 수도 있다). Fake News, Associated Press 등 연이어 나오는 두 단어가 특별한 의미를 가지는 경우 bigram이라고 부른다. 마찬가지로 the Wallstreet Journal, the Washington Post 등 세 단어 이상이 연이어 나와 의미를 가지는 경우 trigram이라고 부른다. 이번 시간에는 fake news와 Associated Press, Washington Post, Fox News, New York Times 등 주요 매체 사이의 상관관계를 확인하기 위해 언론사명을 ngram 처리한다.  
텍스트 자료 전처리에서 반복적으로 강조하는 것처럼, n-gram을 적절하게 처리하기 위해서는 연구자가 분석 자료에 대해 명확히 알고 있어야 한다. 

```{r}
# 트럼프 트윗에서 언론사명에 대한 n-gram 처리
trump_monthly$tweet <- trump_monthly$tweet %>%
  str_replace_all("fake news", "fakenews") %>%
  str_replace_all("(washington post)", "washingtonpost") %>%
  str_replace_all("(associated press)", "associatedpress") %>%
  str_replace_all("(fox news)", "foxnews") %>%
  str_replace_all("(usa today)", "usatoday") %>%
  str_replace_all("(new york times)|(ny times)", "newyorktimes") %>%
  str_replace_all("(wall street journal)", "wallstreetjournal")
```

```{r}
# 트럼프 트윗에서 인명에 대한 n-gram 처리
trump_monthly$tweet <- trump_monthly$tweet %>%
  str_replace_all("(donald trump)|(donald j trump)", "donald trump") %>%
  str_replace_all("(hillari clinton)|(hillari)", "hillariclinton") %>%
  str_replace_all("(barack obama)|(obama)", "barackobama")

```


##### 1.3.7) 불용단어(stop words) 처리

`tm`패키지에는 영어에서 불용단어로 불리는 단어들에 대한 사전이 내장되어 있다. 기본 불용단어 사전으로는 `stopwords("en")`이 있으며, 보다 많은 단어들이 포함된 `stopwards("SMART")`가 있다. 이번 시간에는 SMART 불용단어 사전을 통해 불용단어를 처리하고자 한다. 

```{r}
# 불용단어(stop words) 처리
trump_monthly$tweet <- removeWords(trump_monthly$tweet, stopwords("SMART"))
```


##### 1.3.8) 어근동일화
`tm` 패키지를 통해 어근동일화를 수행할 수 있다. 이는 `tm`패키지를 통해 말뭉치(corpus)를 만든 후 수행한다.


### 2. 텍스트 데이터 기술통계분석

#### 2.1. `tm`패키지를 통한 문서-단어 행렬(DTM)

```{r}
# 문서(월별트윗)-단어(트윗텍스트) 행렬 (document-text matrix, DTM)

trump_monthly <- trump_monthly %>%
  select(month, tweet) %>% as.data.frame()
names(trump_monthly)[1] <- "doc_id"  # dtm을 만들기 위해 월을 문서제목으로 변경
names(trump_monthly)[2] <- "text"    # dtm을 만들기 위해 트윗을 문서 텍스트로 변경

tweet_source <- DataframeSource(trump_monthly)
tweet_corpus <- VCorpus(tweet_source)  # 말뭉치로 만들기

tweet_corpus <- tm_map(tweet_corpus, stripWhitespace)  # 불필요한 여백 제거
tweet_corpus <- tm_map(tweet_corpus, removePunctuation)  # 불필요한 특수문자 제거
tweet_corpus <- tm_map(tweet_corpus, stemDocument, language = "en")  # 어근동일화

tweet_dtm <- DocumentTermMatrix(tweet_corpus)  # DTM
```


```{r}
# 단어의 발현 빈도
word_freq <- apply(tweet_dtm[,], 2, sum)
length(word_freq)  # 전체 고유 단어는 36581개

# 높은 빈도에서 낮은 빈도로 정렬
word_freq_sort <- sort(word_freq, decreasing = TRUE)
word_freq_sort[1:20]

# 누적 빈도 계산
word_freq_sum <- cumsum(word_freq_sort)
word_freq_sum[1:20]

# 비율(전체 합이 1) 계산
word_prop <- word_freq_sum / word_freq_sum[length(word_freq_sum)]
word_prop[1:20]  # 상위 20개의 단어가 전체 트윗 단어의 약 13% 차지
```

#### 2.2. 말구름(word cloud) 만들기
`wordcloud`패키지를 통해 말구름을 그릴 수 있다. 말구름에서 빈도가 높은 단어일 수록 글자 크기가 크고, 가운데에 위치하게 된다. 빈도가 낮은 단어일 수록 글자 크기가 작고 외곽에 위치하게 된다.
`wordcloud()`함수에 앞서 만든 트럼프 트윗의 단어빈도 오브젝트(word_freq)을 입력한다.
`scale = c(2.5, 0.2)`는 가장 큰 글자의 크기가 2.5, 가장 작은 글자의 크기가 0.2임을 의미한다.
`rot.per`를 통해 단어를 90도 회전시킬 수 있는데, 이번에는 회전시키지 않았다.
`min.freq = 400`은 단어 빈도가 400번 미만인 단어들을 표시하지 않는다는 의미이다.
`random.order = FALSE`는 말구름에 단어를 배치할 때 빈도수가 낮은 단어들을 바깥에 배치한다.
```{r}
# 트럼프 트윗 말구름 만들기
# install.packages("wordcloud")
library("wordcloud")
wordcloud(names(word_freq), freq = word_freq, scale = c(2.5, 0.2),
          rot.per = 0.0, min.freq = 400, random.order = FALSE)
```

```{r}
# 색조를 넣은 말구름
library("RColorBrewer")
# display.brewer.all() 을 통해 색깔 팔레트를 볼 수 있다.
color_palette <- brewer.pal(6, "RdGy")
wordcloud(names(word_freq), freq = word_freq, scale = c(2.5, 0.2),
          rot.per = 0.0, min.freq = 400, random.order = FALSE, col = color_palette)
```


#### 2.3. 단어 간 상관관계
DTM은 문서가 가로줄에, 단어가 세로줄에 위치한 행렬이다. 
문서를 일종의 사례(case)로 취급하고, 단어를 변수로 취급한다면 단어i가 등장하였을 때 단어j가 등장하는 연관관계를 계산할 수 있다.  

cf. TDM은 단어가 가로줄에, 문서가 세로줄에 위치한 행렬이다. 이를 이용하면 문서i와 문서j사이의 연관관계를 구할 수 있다. DTM을 전치시키면 TDM을 구할 수 있다.  

`tm`패키지의 `findAssocs()`함수를 통해 단어 간 연관관계를 확인할 수 있다.
예를 들어 워싱턴 포스트("washingtonpost")와 r = 0.60이상 상관관계를 가지는 단어를 알아보자.  

```{r}
# 트럼프의 트윗에서 워싱턴포스트('washingtonpost')와 0.6 이상 상관관계를 가지는 단어
findAssocs(tweet_dtm, "washingtonpost", 0.6)
```

특정한 두 단어 사이의 상관관계도 구할 수 있다. 예를 들어 워싱턴 포스트(washingtonpost)와 가짜뉴스(fakenews)사이의 상관계수는 다음과 같이 구한다.  

```{r}
# washingtonpost 와 fakenews 사이의 피어슨 상관계수 
wp <- as.vector(tweet_dtm[, "washingtonpost"])
fn <- as.vector(tweet_dtm[, "fakenew"])  # 어근 동일화 과정에서 s 삭제
cor.test(wp, fn)
```

```{r}
# 참고: 두 단어 사이의 상관관계를 찾는 함수 만들기
cor_words <- function(dtm, word1, word2) {
  var1 <- as.vector(dtm[, word1])
  var2 <- as.vector(dtm[, word2])
  cor.test(var1, var2)
}

# 생성한 함수를 통해 워싱턴포스트와 가짜뉴스 사이의 상관계수 구하기
cor_words(tweet_dtm, "washingtonpost", "fakenew")
```

문서 간 상관관계
```{r}
# dtm을 전치시켜 tdm으로 만들기.
tweet_tdm <- t(tweet_dtm)  

# 131번째 문서(2020-3월)와 130번째 문서(2020-2월) 간 피어슨 상관계수
cor.test(as.vector(tweet_tdm[, 131]), as.vector(tweet_tdm[, 130]))
```

문서 간 상관계수 매트릭스
```{r}
doc_num <- length(colnames(tweet_tdm))
doc_cor <- matrix(NA, nrow = doc_num, ncol = doc_num)

for (i in 1:doc_num) {
  for (j in 1:doc_num) {
    doc_cor[i,j] <- cor.test(as.vector(tweet_tdm[,i]), as.vector(tweet_tdm[,j]))$est
  }
}
colnames(doc_cor) <- colnames(tweet_tdm)
rownames(doc_cor) <- colnames(tweet_tdm)

# 상관계수 매트릭스의 일부만 살펴보기
doc_cor[1:5, 1:5]

# 소수점 4자리까지만 표시하기
round(doc_cor[125:132, 125:132], 4)
```


문서 간 상관계수 히스토그램
```{r}
summary(doc_cor[lower.tri(doc_cor)])

hist(doc_cor[lower.tri(doc_cor)], breaks = 30,
     xlim = c(-0.2, 1.0),
     col = 'lightblue',
     xlab = "Correlations",
     main = "Correlations between Trump's Monthly Tweet")

```



#### 2.4 위계적 군집분석
군집분석은 주어진 변수들을 기준으로 유사한(인접한) 사례를 묶어 자료를 탐색하는 통계기법이다. 유사한 사례를 묶어 군집을 형성하는 덴드로그램(dendrogram)을 통해 시각화 할 수 있다.  
이번 시간에는 트럼프의 월별 트윗에 대한 DTM을 군집화 한다. 군집분석은 먼저 사례들(DTM에서는 문서들) 사이의 유사도를 계산한다. 유사도는 `dist()`함수를 통해 유클리드 거리(Euclidean Distance, 두 점 사이의 직선거리)를 계산하여 구한다. 숫자가 작을 수록 문서가 유사하고, 숫자가 클 수록 문서가 유사하지 않다.  

```{r}
# 문서 간 유사도 행렬
dist_tweet_dtm <- dist(tweet_dtm)  # 유클리드 거리 계산
as.matrix(dist_tweet_dtm)[1:5, 1:5]  # 값이 작을 수록 유사함.
```

`hclust()`함수를 이용하여 위의 유사도 행렬(`dist_tweet_dtm`)에 대한 위계적 군집 분석을 실시할 수 있다. 군집분석의 방법은 대개 관례나 연구자의 주관적 판단에 따라 선택된다. 여기서는 Ward의 방법을 이용하여 군집분석을 실시한다.

```{r}
# Ward의 방법으로 군집분석 실시
tweet_clusters <- hclust(dist_tweet_dtm, method = "ward.D2")
plot(tweet_clusters)
```

최종적으로 형성될 군집의 수를 `cutree(object, k = n)` 함수의 옵션을 통해 조정할 수 있다. 트럼프 대통령의 월별 트윗에 대한 최종 군집의 수를 5개로 결정하면 다음과 같은 결과를 얻는다.  

```{r}
# 최종 군집의 수 변경
tweet_group <- cutree(tweet_clusters, k = 5)
tweet_group
```

만약 군집의 수가 너무 적어서 트럼프 트윗에서 나타나는 월별 다양성을 충분히 반영하지 못한다고 생각한다면, 최종 군집의 수를 아래와 같이 조정할 수 있다.

```{r}
tweet_group <- cutree(tweet_clusters, k = 8)
tweet_group
```


군집별로 다른 색을 적용하여 시각화하는 방법은 다음과 같다. 이 방법은 `dendextend` 라이브러리를 필요로 한다. 

```{r}
# 군집별로 다른 색을 적용하여 그래프로 그리기.
library("dendextend")  # 패키지 부착

dend <- as.dendrogram(tweet_clusters)
tweet_k <- 8  # 클러스터의 개수 8개

dend <- dend %>%
  color_branches(k = tweet_k) %>%
  color_labels(dend, k = tweet_k) %>%
  set("branches_lwd", 2) %>%
  set("branches_lty", 1)

plot(dend, main = "Clustering Monthly Tweets", ylab = "Height")
```

이번 시간 예시로 사용하는 트럼프의 트윗은 2009년 5월부터 2020년 4월까지 총 132개월의 트윗을 모은 문서로 구성되어 있기 때문에, 덴드로그램의 라벨을 알아보기 쉽지 않다. 따라서 트럼프 트윗을 8개로 군집화 하였을때, 군집의 분포를 연도별로 나타내는 빈도표를 구해보고자 한다.  
먼저 트윗의 클러스터 오브젝트(`tweet_clusters`)에서 문서 제목에 해당하는 라벨(labels) 변수는 연-월(YYYY-MM) 형태로 표시되어 있다(`tweet_clusters$labels`로 확인 가능). 여기서 처음 숫자 4개를 추출하면 연도를 알 수 있다. 따라서 교차표의 한 축에는 `str_extract()`함수를 통해 연도를 추출하고, 다른 축에는 `cutree(tweet_clusters, k = 8)`함수를 통해 8개의 클러스터를 표시한다.

```{r}
# 연도 별 군집 분포의 빈도 구하기
tweet_table <- table(str_extract(tweet_clusters$labels, "[[:digit:]]{4}"),
                     cutree(tweet_clusters, k = 8))
tweet_table
```

교차표의 세로축은 연도를 나타낸다. 연도별로 12개의 트윗 문서(단, 2009년, 2020년 예외)가 존재하므로, 각 행의 합은 12가 되어야 한다(2009년 8개, 2020년 4개). 교차표의 가로축은 클러스터를 의미한다. 따라서 교차표의 각 열의 합은 해당 클러스터에 포함된 월별 트윗 문서의 수를 의미한다.  
교차표를 살펴보면 2009년부터 2011년까지 트럼프 대통령의 모든 트윗은 1번 클러스터에 포함되어 있다. 2012년부터는 대개 2~4개 사이의 클러스터에 해당하는 트윗이 월별로 작성되었다.  
물론 앞서 언급한 바와 같이 개별적인 트윗은 하나의 독립된 문서로 보는 것이 타당할 수도 있다. 즉 트윗을 월별로 묶는 행위는 많은 정보를 상실하게 한다. 다만 트럼프 트위터에서 나타나는 월별 경향을 살펴보고자 하는 연구자에게는 이와 같은 분류가 도움이 될 수도 있다.  
위의 교차표를 막대그래프로 나타내어 트럼프의 월별 트윗 클러스터 분포를 연도별로 표시할 수 있다.

```{r}
# 연도별 군집의 막대그래프
library("ggplot2")  # ggplot2 라이브러리 부착

# ggplot2의 사용을 위해 longform으로 데이터 변경 
cluster_year <- data.frame(tweet_table) 
# 변수병 변경
colnames(cluster_year) <- c("year", "cluster", "tweet")  
# 클러스터 변수를 구체화 하여 표시 (cluster1, cluset2,...,cluster8)
cluster_year$cluster <- paste("cluster", cluster_year$cluster)  
# year 변수를 factor에서 numeric으로 형변환
cluster_year$year <- as.numeric(as.character(cluster_year$year))  

# 트럼프 트윗의 클러스터 분포를 연도별 막대그래프로 표시
ggplot(data = cluster_year, aes(x = year, y = tweet, fill = cluster)) +
  geom_bar(stat = "identity") +
  scale_x_continuous(breaks = 2009:2020, labels = 2009:2020) +
  scale_y_continuous(breaks = 0:12, labels = 0:12)
```


